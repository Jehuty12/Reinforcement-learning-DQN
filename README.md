"# Projet d'Apprentissage par Renforcement - DQN

Ce projet impl√©mente diff√©rents algorithmes d'apprentissage par renforcement, de l'exploration basique avec Q-Learning jusqu'aux r√©seaux de neurones Deep Q-Network (DQN) avanc√©s.

## üìã Description du Projet

Le projet est organis√© en 3 √©tapes progressives :

### Step 1 : Frozen Lake - Q-Learning
- **Environnement** : FrozenLake (Gymnasium)
- **Algorithme** : Q-Learning avec exploration Œµ-greedy adaptative
- **Objectifs** : Apprendre les bases de l'apprentissage par renforcement
- **Fichiers principaux** :
  - `frozen_lake.py` : Impl√©mentation principale avec Q-Learning adaptatif
  - `frozen_lake.ipynb` : Notebook Jupyter pour l'analyse
  - `FrozenLake_tuto.py` : Version tutoriel

### Step 2 : Cliff Walking - SARSA
- **Environnement** : CliffWalking (Gymnasium)
- **Algorithme** : SARSA (State-Action-Reward-State-Action)
- **Objectifs** : Comprendre la diff√©rence entre on-policy et off-policy
- **Fichiers principaux** :
  - `cliff.py` : Impl√©mentation SARSA
  - `cliff_sarsa.py` : Version alternative

### Step 3 : Atari Games - Deep Q-Network (DQN)
- **Environnement** : Jeux Atari (ALE/Pong-v5 par d√©faut)
- **Algorithme** : DQN avec replay buffer et target network
- **Fonctionnalit√©s avanc√©es** :
  - Experience Replay Buffer
  - Target Network
  - Preprocessing des images
  - Prioritized Experience Replay
- **Fichiers principaux** :
  - `run.py` : Point d'entr√©e pour l'entra√Ænement
  - `trainer.py` : Logique d'entra√Ænement principal
  - `model.py` : Architecture du r√©seau de neurones
  - `replay_buffer.py` : Buffer d'exp√©rience
  - `preprocessing.py` : Pr√©processing des observations
  - `inference.py` : √âvaluation du mod√®le entra√Æn√©

## üöÄ Installation et Configuration

### Pr√©requis
- Python 3.8+
- CUDA (optionnel, pour l'acc√©l√©ration GPU)

### Option 1 : Installation avec pip (standard)

```bash
# Naviguer vers le dossier du projet
cd "Renforcement Learning"

# Installer les d√©pendances
pip install -r requirement.txt
```

### Option 2 : Installation avec Miniconda (recommand√©)

Si vous pr√©f√©rez utiliser un environnement virtuel isol√© avec Miniconda :

#### Installation de Miniconda

1. **T√©l√©charger Miniconda** depuis [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)
2. **Installer Miniconda** en suivant les instructions pour votre OS
3. **Red√©marrer votre terminal** ou ex√©cuter `conda init`

#### Configuration de l'environnement

```bash
# Cr√©er un nouvel environnement Python 3.9
conda create -n dqn-env python=3.9

# Activer l'environnement
conda activate dqn-env

# Naviguer vers le dossier du projet
cd "Renforcement Learning"

# Installer les d√©pendances principales via conda (recommand√©)
conda install pytorch torchvision numpy matplotlib opencv -c pytorch -c conda-forge

# Installer les d√©pendances restantes via pip
pip install gymnasium ale-py farama-notifications

# Ou installer toutes les d√©pendances via pip
# pip install -r requirement.txt
```

#### Utilisation quotidienne avec Miniconda

```bash
# Activer l'environnement √† chaque session
conda activate dqn-env

# Lancer vos scripts
python run.py

# D√©sactiver l'environnement quand termin√©
conda deactivate
```

#### Avantages de Miniconda
- **Isolation compl√®te** : Aucun conflit avec d'autres projets Python
- **Gestion simplifi√©e** : Installation automatique des d√©pendances syst√®me
- **Performance optimis√©e** : Versions optimis√©es des biblioth√®ques scientifiques
- **Portabilit√©** : Facilite le partage et la reproduction de l'environnement

### D√©pendances principales
- **gymnasium** : Environnements de jeu
- **torch** : Framework de deep learning
- **numpy** : Calculs num√©riques
- **matplotlib** : Visualisation
- **opencv-python** : Traitement d'images
- **ale-py** : Arcade Learning Environment pour Atari

## üìö Utilisation

> **Note** : Si vous utilisez Miniconda, n'oubliez pas d'activer votre environnement avant de lancer les scripts :
> ```bash
> conda activate dqn-env
> ```

### Step 1 : Frozen Lake (Q-Learning)

```bash
# Naviguer vers Step_1
cd "Step_1"

# Lancer l'entra√Ænement basique
python frozen_lake.py

# Ou utiliser le notebook Jupyter
jupyter notebook frozen_lake.ipynb
```

### Step 2 : Cliff Walking (SARSA)

```bash
# Naviguer vers Step_2
cd "Step_2"

# Lancer l'entra√Ænement SARSA
python cliff.py
```

### Step 3 : DQN pour Atari

```bash
# Naviguer vers Step_3
cd "Step_3"

# Entra√Ænement avec param√®tres par d√©faut (Pong)
python run.py

# Entra√Ænement personnalis√©
python run.py --env "ALE/Breakout-v5" --total-steps 10000000 --lr 0.0001

# √âvaluation d'un mod√®le entra√Æn√©
python inference.py --model-path "checkpoints/dqn_pong.pth" --env "ALE/Pong-v5"
```

### Param√®tres disponibles pour DQN

| Param√®tre | Description | Valeur par d√©faut |
|-----------|-------------|-------------------|
| `--env` | Environnement Atari | "ALE/Pong-v5" |
| `--total-steps` | Nombre total d'√©tapes | 50,000,000 |
| `--buffer-cap` | Taille du replay buffer | 500,000 |
| `--batch-size` | Taille des mini-batches | 32 |
| `--gamma` | Facteur de discount | 0.99 |
| `--train-freq` | Fr√©quence d'entra√Ænement | 4 |
| `--target-update` | Fr√©quence de mise √† jour du target network | 10,000 |
| `--lr` | Taux d'apprentissage | 0.00025 |
| `--initial-replay` | √âtapes avant l'entra√Ænement | 50,000 |
| `--resume` | Reprendre depuis un checkpoint | None |
| `--save-path` | Chemin de sauvegarde | "checkpoints/dqn_breakout.pth" |

## üìä R√©sultats et M√©triques

Le projet g√©n√®re automatiquement :
- **Graphiques de performance** : R√©compenses moyennes, scores
- **Vid√©os de gameplay** : Enregistrement p√©riodique des parties
- **Checkpoints** : Sauvegarde des mod√®les entra√Æn√©s
- **Logs d'entra√Ænement** : M√©triques d√©taill√©es

## üèóÔ∏è Architecture du Projet

```
DQN/
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ Renforcement Learning/
    ‚îú‚îÄ‚îÄ requirement.txt
    ‚îú‚îÄ‚îÄ Step_1/                 # Q-Learning sur FrozenLake
    ‚îÇ   ‚îú‚îÄ‚îÄ frozen_lake.py
    ‚îÇ   ‚îú‚îÄ‚îÄ frozen_lake.ipynb
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îú‚îÄ‚îÄ Step_2/                 # SARSA sur CliffWalking
    ‚îÇ   ‚îú‚îÄ‚îÄ cliff.py
    ‚îÇ   ‚îî‚îÄ‚îÄ cliff_sarsa.py
    ‚îî‚îÄ‚îÄ Step_3/                 # DQN pour Atari
        ‚îú‚îÄ‚îÄ run.py              # Point d'entr√©e
        ‚îú‚îÄ‚îÄ trainer.py          # Logique d'entra√Ænement
        ‚îú‚îÄ‚îÄ model.py            # Architecture CNN
        ‚îú‚îÄ‚îÄ replay_buffer.py    # Experience replay
        ‚îú‚îÄ‚îÄ preprocessing.py    # Pr√©processing images
        ‚îú‚îÄ‚îÄ inference.py        # √âvaluation
        ‚îî‚îÄ‚îÄ ...
```

## üî¨ Concepts Impl√©ment√©s

- **Q-Learning** : Apprentissage par diff√©rence temporelle
- **Œµ-greedy exploration** : Balance exploration/exploitation
- **SARSA** : Algorithme on-policy
- **Deep Q-Network (DQN)** : Q-Learning avec r√©seaux de neurones
- **Experience Replay** : R√©utilisation des exp√©riences pass√©es
- **Target Network** : Stabilisation de l'entra√Ænement
- **Preprocessing** : Normalisation et redimensionnement des images

## üéØ Environnements Support√©s

### Step 1 & 2
- FrozenLake-v1
- CliffWalking-v0

### Step 3 (Atari)
- ALE/Pong-v5
- ALE/Breakout-v5
- ALE/SpaceInvaders-v5
- Tous les jeux Atari compatibles avec ALE

## üõ†Ô∏è D√©pannage

### Probl√®mes courants

1. **Erreur CUDA** : V√©rifiez l'installation de PyTorch avec support CUDA
2. **Environnement manquant** : Installez `ale-py` pour les jeux Atari
3. **M√©moire insuffisante** : R√©duisez `buffer-cap` ou `batch-size`
4. **Conflits de d√©pendances** : Utilisez Miniconda pour un environnement isol√©
5. **Erreur "conda command not found"** : Red√©marrez votre terminal apr√®s l'installation de Miniconda

### Commandes utiles avec Miniconda

```bash
# Lister tous les environnements
conda env list

# Supprimer un environnement
conda env remove -n dqn-env

# Exporter l'environnement pour le partage
conda env export > environment.yml

# Cr√©er un environnement depuis un fichier
conda env create -f environment.yml
```

### Performance

- **GPU recommand√©** pour Step 3 (DQN) ou via un serveur cloud ( ex. Google Colab, AWS, kaggle, etc.)
- **RAM minimum** : 8GB pour les grands replay buffers
- **Temps d'entra√Ænement** : Plusieurs heures √† jours selon l'environnement

## üìÑ Licence

Ce projet est √† des fins √©ducatives et de recherche.

**Note** : Ce projet suit une progression p√©dagogique du simple (Q-Learning) au complexe (DQN). Il est recommand√© de suivre les √©tapes dans l'ordre pour une meilleure compr√©hension." 
