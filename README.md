"# Projet d'Apprentissage par Renforcement - DQN

Ce projet implÃ©mente diffÃ©rents algorithmes d'apprentissage par renforcement, de l'exploration basique avec Q-Learning jusqu'aux rÃ©seaux de neurones Deep Q-Network (DQN) avancÃ©s.

## ğŸ“‹ Description du Projet

Le projet est organisÃ© en 3 Ã©tapes progressives :

### Step 1 : Frozen Lake - Q-Learning
- **Environnement** : FrozenLake (Gymnasium)
- **Algorithme** : Q-Learning avec exploration Îµ-greedy adaptative
- **Objectifs** : Apprendre les bases de l'apprentissage par renforcement
- **Fichiers principaux** :
  - `frozen_lake.py` : ImplÃ©mentation principale avec Q-Learning adaptatif
  - `frozen_lake.ipynb` : Notebook Jupyter pour l'analyse
  - `FrozenLake_tuto.py` : Version tutoriel

### Step 2 : Cliff Walking - SARSA
- **Environnement** : CliffWalking (Gymnasium)
- **Algorithme** : SARSA (State-Action-Reward-State-Action)
- **Objectifs** : Comprendre la diffÃ©rence entre on-policy et off-policy
- **Fichiers principaux** :
  - `cliff.py` : ImplÃ©mentation SARSA
  - `cliff_sarsa.py` : Version alternative

### Step 3 : Atari Games - Deep Q-Network (DQN)
- **Environnement** : Jeux Atari (ALE/Pong-v5 par dÃ©faut)
- **Algorithme** : DQN avec replay buffer et target network
- **FonctionnalitÃ©s avancÃ©es** :
  - Experience Replay Buffer
  - Target Network
  - Preprocessing des images
  - Prioritized Experience Replay
- **Fichiers principaux** :
  - `run.py` : Point d'entrÃ©e pour l'entraÃ®nement
  - `trainer.py` : Logique d'entraÃ®nement principal
  - `model.py` : Architecture du rÃ©seau de neurones
  - `replay_buffer.py` : Buffer d'expÃ©rience
  - `preprocessing.py` : PrÃ©processing des observations
  - `inference.py` : Ã‰valuation du modÃ¨le entraÃ®nÃ©

## ğŸš€ Installation et Configuration

### PrÃ©requis
- Python 3.8+
- CUDA (optionnel, pour l'accÃ©lÃ©ration GPU)

### Installation des dÃ©pendances

```bash
# Naviguer vers le dossier du projet
cd "Renforcement Learning"

# Installer les dÃ©pendances
pip install -r requirement.txt
```

### DÃ©pendances principales
- **gymnasium** : Environnements de jeu
- **torch** : Framework de deep learning
- **numpy** : Calculs numÃ©riques
- **matplotlib** : Visualisation
- **opencv-python** : Traitement d'images
- **ale-py** : Arcade Learning Environment pour Atari

## ğŸ“š Utilisation

### Step 1 : Frozen Lake (Q-Learning)

```bash
# Naviguer vers Step_1
cd "Step_1"

# Lancer l'entraÃ®nement basique
python frozen_lake.py

# Ou utiliser le notebook Jupyter
jupyter notebook frozen_lake.ipynb
```

### Step 2 : Cliff Walking (SARSA)

```bash
# Naviguer vers Step_2
cd "Step_2"

# Lancer l'entraÃ®nement SARSA
python cliff.py
```

### Step 3 : DQN pour Atari

```bash
# Naviguer vers Step_3
cd "Step_3"

# EntraÃ®nement avec paramÃ¨tres par dÃ©faut (Pong)
python run.py

# EntraÃ®nement personnalisÃ©
python run.py --env "ALE/Breakout-v5" --total-steps 10000000 --lr 0.0001

# Ã‰valuation d'un modÃ¨le entraÃ®nÃ©
python inference.py --model-path "checkpoints/dqn_pong.pth" --env "ALE/Pong-v5"
```

### ParamÃ¨tres disponibles pour DQN

| ParamÃ¨tre | Description | Valeur par dÃ©faut |
|-----------|-------------|-------------------|
| `--env` | Environnement Atari | "ALE/Pong-v5" |
| `--total-steps` | Nombre total d'Ã©tapes | 50,000,000 |
| `--buffer-cap` | Taille du replay buffer | 500,000 |
| `--batch-size` | Taille des mini-batches | 32 |
| `--gamma` | Facteur de discount | 0.99 |
| `--train-freq` | FrÃ©quence d'entraÃ®nement | 4 |
| `--target-update` | FrÃ©quence de mise Ã  jour du target network | 10,000 |
| `--lr` | Taux d'apprentissage | 0.00025 |
| `--initial-replay` | Ã‰tapes avant l'entraÃ®nement | 50,000 |
| `--resume` | Reprendre depuis un checkpoint | None |
| `--save-path` | Chemin de sauvegarde | "checkpoints/dqn_breakout.pth" |

## ğŸ“Š RÃ©sultats et MÃ©triques

Le projet gÃ©nÃ¨re automatiquement :
- **Graphiques de performance** : RÃ©compenses moyennes, scores
- **VidÃ©os de gameplay** : Enregistrement pÃ©riodique des parties
- **Checkpoints** : Sauvegarde des modÃ¨les entraÃ®nÃ©s
- **Logs d'entraÃ®nement** : MÃ©triques dÃ©taillÃ©es

## ğŸ—ï¸ Architecture du Projet

```
DQN/
â”œâ”€â”€ README.md
â””â”€â”€ Renforcement Learning/
    â”œâ”€â”€ requirement.txt
    â”œâ”€â”€ Step_1/                 # Q-Learning sur FrozenLake
    â”‚   â”œâ”€â”€ frozen_lake.py
    â”‚   â”œâ”€â”€ frozen_lake.ipynb
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ Step_2/                 # SARSA sur CliffWalking
    â”‚   â”œâ”€â”€ cliff.py
    â”‚   â””â”€â”€ cliff_sarsa.py
    â””â”€â”€ Step_3/                 # DQN pour Atari
        â”œâ”€â”€ run.py              # Point d'entrÃ©e
        â”œâ”€â”€ trainer.py          # Logique d'entraÃ®nement
        â”œâ”€â”€ model.py            # Architecture CNN
        â”œâ”€â”€ replay_buffer.py    # Experience replay
        â”œâ”€â”€ preprocessing.py    # PrÃ©processing images
        â”œâ”€â”€ inference.py        # Ã‰valuation
        â””â”€â”€ ...
```

## ğŸ”¬ Concepts ImplÃ©mentÃ©s

- **Q-Learning** : Apprentissage par diffÃ©rence temporelle
- **Îµ-greedy exploration** : Balance exploration/exploitation
- **SARSA** : Algorithme on-policy
- **Deep Q-Network (DQN)** : Q-Learning avec rÃ©seaux de neurones
- **Experience Replay** : RÃ©utilisation des expÃ©riences passÃ©es
- **Target Network** : Stabilisation de l'entraÃ®nement
- **Preprocessing** : Normalisation et redimensionnement des images

## ğŸ¯ Environnements SupportÃ©s

### Step 1 & 2
- FrozenLake-v1
- CliffWalking-v0

### Step 3 (Atari)
- ALE/Pong-v5
- ALE/Breakout-v5
- ALE/SpaceInvaders-v5
- Tous les jeux Atari compatibles avec ALE

## ğŸ› ï¸ DÃ©pannage

### ProblÃ¨mes courants

1. **Erreur CUDA** : VÃ©rifiez l'installation de PyTorch avec support CUDA
2. **Environnement manquant** : Installez `ale-py` pour les jeux Atari
3. **MÃ©moire insuffisante** : RÃ©duisez `buffer-cap` ou `batch-size`

### Performance

- **GPU recommandÃ©** pour Step 3 (DQN) ou via un serveur cloud ( ex. Google Colab, AWS, kaggle, etc.)
- **RAM minimum** : 8GB pour les grands replay buffers
- **Temps d'entraÃ®nement** : Plusieurs heures Ã  jours selon l'environnement

## ğŸ“„ Licence

Ce projet est Ã  des fins Ã©ducatives et de recherche.

**Note** : Ce projet suit une progression pÃ©dagogique du simple (Q-Learning) au complexe (DQN). Il est recommandÃ© de suivre les Ã©tapes dans l'ordre pour une meilleure comprÃ©hension." 
